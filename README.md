# Qiskit Chatbot

This project uses two LLMs— An Ollama model and GPT—to collaboratively generate and refine qiskit code.

## How It Works
1. **LLM A (Qiskit Code Assitant):** Generates the initial code based on a given prompt.
2. **LLM B (o1-preview):** Reviews and refines the code for correctness, efficiency, and readability.

## Setup

1. Clone the repository:

   ```bash
   git clone https://github.com/Balaji-Udayagiri/Qiskit-Chatbot.git
   cd Qiskit-Chatbot
2. Install dependencies:
   ```bash
   pip install -r requirements.txt
## Usage

`--task_file` (required): Path to the JSON file containing the prompts. Each prompt should be an object with a task_id and prompt field.

`--ollama_model` (optional): The model to use for Ollama code generation. If not specified, the default model `hf.co/Qiskit/granite-8b-qiskit-GGUF:latest` will be used.

`--gpt_model` (optional): The model to use for GPT code refinement. If not specified, the default model o1-preview will be used.

`--initial` (optional): Flag to evaluate only the initial code generated by Ollama.

`--refined` (optional): Flag to evaluate only the refined code generated by GPT.

## Example Usage

1. To evaluate only the initial code generated by Ollama:
   ```bash
   python main.py --task_file "prompts/tasks.json" --initial

2. Evaluate Refined Code Only:
   ```bash
   python main.py --task_file "prompts/tasks.json" --refined

3. Evaluate Both Initial and Refined Code (default behavior):
   ```bash
   python main.py --task_file "prompts/tasks.json"


4. To specify custom models for Ollama and GPT:
   ```bash
   python main.py --task_file "prompts/tasks.json" --ollama_model "Ollama_Model" --gpt_model "GPTModel"

## Output

The results will be saved in a CSV file (outputs/evaluation_results.csv), with the following columns:
`task_id`: The ID of the task being processed.
`difficulty_Scale`: The difficulty scale from the task JSON.
`result`: The result of the evaluation (PASS or FAIL).
`error_message`: Any error message encountered during evaluation.

